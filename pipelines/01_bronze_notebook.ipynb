{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad50e5d2-6310-4ad3-a039-f855d96bf4a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Camada Bronze - Padronização SIAPE\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivo\n",
    "Este notebook é responsável por ingerir os arquivos CSV brutos armazenados na Staging Area (Volumes), aplicar um tratamento estrutural primário de esquema e salvá-los como Tabelas Gerenciadas no formato **Delta Lake**. O foco da Camada Bronze é manter os dados em sua granularidade original (raw), adicionando apenas metadados de governança.\n",
    "\n",
    "## Decisões de Engenharia e Governança\n",
    "1. **Tratamento Dinâmico de Esquema:** Os cabeçalhos originais do governo possuem caracteres inválidos para o formato Parquet/Delta (espaços, parênteses, etc.). A função `sanitize_column_names` utiliza Regex para higienizar todas as colunas de forma escalável e independente do domínio.\n",
    "2. **Data Lineage (Rastreabilidade):** Atendendo ao requisito do desafio, foi incorporada a coluna `arquivo_origem` (utilizando metadados nativos do Spark `_metadata.file_path`) para garantir a rastreabilidade exata do arquivo de origem de cada registro.\n",
    "3. **Encoding de Legado:** Configuração explícita de leitura em `ISO-8859-1` e separador `;` para garantir a integridade de caracteres da língua portuguesa.\n",
    "4. **Native Partition Discovery:** O pipeline reconhece automaticamente a estrutura física de pastas `posicao_base=YYYYMM` gerada na etapa de Extract & Load, particionando o Delta Lake de forma otimizada para consultas temporais.\n",
    "\n",
    "---\n",
    "**Origem:** Arquivos CSV particionados (`/Volumes/workspace/default/staging-siape-tables`)\n",
    "\n",
    "**Destino:** Delta Tables Gerenciadas (`public_informations.bronze_...`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67dfd13c-2288-4fbd-9f91-08bacfbce32e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 22:01:07,653 - Iniciando processamento da Camada Bronze...\n2026-02-25 22:01:07,653 - Processando domínio: CADASTRO\n2026-02-25 22:01:48,627 -  -> ✅ Tabela public_informations.bronze_cadastro catalogada com sucesso!\n2026-02-25 22:01:48,627 - Processando domínio: REMUNERACAO\n2026-02-25 22:02:00,577 -  -> ✅ Tabela public_informations.bronze_remuneracao catalogada com sucesso!\n2026-02-25 22:02:00,577 - Processando domínio: AFASTAMENTOS\n2026-02-25 22:02:06,417 -  -> ✅ Tabela public_informations.bronze_afastamentos catalogada com sucesso!\n2026-02-25 22:02:06,417 - Processando domínio: OBSERVACOES\n2026-02-25 22:02:12,255 -  -> ✅ Tabela public_informations.bronze_observacoes catalogada com sucesso!\n2026-02-25 22:02:12,256 - Camada Bronze finalizada!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import logging\n",
    "import re\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "VOLUME_PATH = \"/Volumes/workspace/default/staging-siape-tables\"\n",
    "SCHEMA_NAME = \"public_informations\"\n",
    "DOMAINS = [\"cadastro\", \"remuneracao\", \"afastamentos\", \"observacoes\"]\n",
    "\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {SCHEMA_NAME}\")\n",
    "\n",
    "def sanitize_column_names(df):\n",
    "    \"\"\"\n",
    "    Remove caracteres inválidos do Delta Lake e substitui espaços por underline.\n",
    "    Exemplo: 'REMUNERAÇÃO BÁSICA (R$)' vira 'REMUNERAÇÃO_BÁSICA_R$'\n",
    "    \"\"\"\n",
    "    for col_name in df.columns:\n",
    "        # Substitui os caracteres proibidos do Delta e espaços por '_'\n",
    "        clean_name = re.sub(r'[ ,;{}()\\n\\t=]+', '_', col_name)\n",
    "        # Remove underlines duplicados ou no final\n",
    "        clean_name = re.sub(r'_+', '_', clean_name).strip('_')\n",
    "        \n",
    "        df = df.withColumnRenamed(col_name, clean_name)\n",
    "    return df\n",
    "\n",
    "def run_bronze_from_partitioned_volumes():\n",
    "    for domain in DOMAINS:\n",
    "        logger.info(f\"Processando domínio: {domain.upper()}\")\n",
    "        \n",
    "        source_path = f\"{VOLUME_PATH}/{domain}/\"\n",
    "        table_name = f\"{SCHEMA_NAME}.bronze_{domain}\"\n",
    "        \n",
    "        try:\n",
    "            df_raw = (spark.read\n",
    "                      .format(\"csv\")\n",
    "                      .option(\"header\", \"true\")\n",
    "                      .option(\"sep\", \";\")\n",
    "                      .option(\"encoding\", \"ISO-8859-1\")\n",
    "                      .load(source_path))\n",
    "            \n",
    "            df_clean_cols = sanitize_column_names(df_raw)\n",
    "            \n",
    "            df_bronze = (df_clean_cols\n",
    "                         .withColumn(\"arquivo_origem\", F.col(\"_metadata.file_path\"))\n",
    "                         .withColumn(\"timestamp_carga_bronze\", F.current_timestamp()))\n",
    "            \n",
    "            (df_bronze.write\n",
    "             .format(\"delta\")\n",
    "             .mode(\"overwrite\") \n",
    "             .partitionBy(\"posicao_base\")\n",
    "             .saveAsTable(table_name))\n",
    "            \n",
    "            logger.info(f\" -> ✅ Tabela {table_name} catalogada com sucesso!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\" -> ❌ Erro ao processar {domain.upper()}: {e}\", exc_info=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Iniciando processamento da Camada Bronze...\")\n",
    "    run_bronze_from_partitioned_volumes()\n",
    "    logger.info(\"Camada Bronze finalizada!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4560000445758603,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}